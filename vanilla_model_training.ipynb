{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff720e7",
   "metadata": {},
   "source": [
    "Cell 1 – Imports & basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e7c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296e068",
   "metadata": {},
   "source": [
    "Cell 2 – Tokenizer & corpus loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d51c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Very basic tokenizer:\n",
    "    - lowercase\n",
    "    - keep only alphabetic characters and spaces\n",
    "    - split on whitespace\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\\\s]\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path: Path):\n",
    "    \"\"\"\n",
    "    Load corpus as list of token lists (one per line).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with corpus_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens = simple_tokenize(line)\n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "    print(f\"Loaded {len(sentences)} sentences.\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a19a94",
   "metadata": {},
   "source": [
    "Cell 3 – Word2VecScratch class (batched skip-gram + negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c849d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecScratch:\n",
    "    def __init__( # default hardcoded values\n",
    "        self,\n",
    "        embedding_dim=100,\n",
    "        window_size=2,          # up to 2 words on each side\n",
    "        min_count=1,\n",
    "        negative_samples=5,\n",
    "        lr=0.025,\n",
    "        epochs=3,\n",
    "        batch_size=4,           # mini-batch size\n",
    "        seed=42,\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.min_count = min_count\n",
    "        self.negative_samples = negative_samples\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # will be set in build_vocab\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.vocab_size = 0\n",
    "\n",
    "        # will be set in init_weights\n",
    "        self.W_in = None   # shape (vocab_size, embedding_dim)\n",
    "        self.W_out = None  # shape (vocab_size, embedding_dim)\n",
    "\n",
    "        # negative sampling distribution\n",
    "        self.neg_sampling_probs = None\n",
    "\n",
    "    # ---------- PREP: VOCAB + TRAINING PAIRS ----------\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of token lists.\n",
    "        \"\"\"\n",
    "        counts = Counter()\n",
    "        for sent in sentences:\n",
    "            counts.update(sent)\n",
    "\n",
    "        # filter by min_count\n",
    "        filtered = [w for w, c in counts.items() if c >= self.min_count]\n",
    "\n",
    "        self.idx2word = sorted(filtered)\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.idx2word)}\n",
    "        self.vocab_size = len(self.idx2word)\n",
    "        print(f\"Vocab size: {self.vocab_size}\")\n",
    "\n",
    "        # build negative sampling distribution: P(w) is proportional to count(w)^0.75\n",
    "        freqs = np.array([counts[w] for w in self.idx2word], dtype=np.float64)\n",
    "        freqs = freqs ** 0.75\n",
    "        self.neg_sampling_probs = freqs / freqs.sum()\n",
    "\n",
    "    def sentences_to_indices(self, sentences):\n",
    "        \"\"\"\n",
    "        Map tokens to indices, dropping out-of-vocab words.\n",
    "        \"\"\"\n",
    "        idx_sentences = []\n",
    "        for sent in sentences:\n",
    "            idxs = [self.word2idx.get(w) for w in sent if w in self.word2idx]\n",
    "            if len(idxs) > 1:\n",
    "                idx_sentences.append(idxs)\n",
    "        return idx_sentences\n",
    "\n",
    "    def generate_skipgram_pairs(self, idx_sentences):\n",
    "        \"\"\"\n",
    "        Generate (center, context) index pairs for skip-gram.\n",
    "        window_size = 2 means up to 2 words on each side of the center word.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        w = self.window_size\n",
    "        for sent in idx_sentences:\n",
    "            n = len(sent)\n",
    "            for i, center in enumerate(sent):\n",
    "                start = max(0, i - w)\n",
    "                end = min(n, i + w + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    context = sent[j]\n",
    "                    pairs.append((center, context))\n",
    "        return pairs\n",
    "\n",
    "    # ---------- MODEL INIT ----------\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialise input and output embeddings.\n",
    "        \"\"\"\n",
    "        self.W_in = 0.01 * self.rng.standard_normal(\n",
    "            (self.vocab_size, self.embedding_dim)\n",
    "        )\n",
    "        self.W_out = 0.01 * self.rng.standard_normal(\n",
    "            (self.vocab_size, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "    # ---------- NEGATIVE SAMPLING + TRAINING ----------\n",
    "\n",
    "    def sample_negatives(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample negative word indices according to the unigram^0.75 distribution.\n",
    "        Returns shape (batch_size, negative_samples).\n",
    "        \"\"\"\n",
    "        return self.rng.choice(\n",
    "            self.vocab_size,\n",
    "            size=batch_size * self.negative_samples,\n",
    "            replace=True,\n",
    "            p=self.neg_sampling_probs,\n",
    "        ).reshape(batch_size, self.negative_samples)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"\n",
    "        High-level training:\n",
    "        - build vocab\n",
    "        - convert sentences to indices\n",
    "        - generate skip-gram pairs\n",
    "        - train using negative sampling with mini-batches\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Starting Word2Vec Training\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\n[Step 1/4] Building vocabulary...\")\n",
    "        self.build_vocab(sentences)\n",
    "        \n",
    "        print(\"\\n[Step 2/4] Converting sentences to indices...\")\n",
    "        idx_sentences = self.sentences_to_indices(sentences)\n",
    "        \n",
    "        print(\"\\n[Step 3/4] Generating skip-gram pairs...\")\n",
    "        pairs = self.generate_skipgram_pairs(idx_sentences)\n",
    "        print(f\"Generated {len(pairs):,} training pairs\")\n",
    "\n",
    "        print(\"\\n[Step 4/4] Initializing weights...\")\n",
    "        self.init_weights()\n",
    "\n",
    "        pairs = np.array(pairs, dtype=np.int64)\n",
    "        n_pairs = len(pairs)\n",
    "        n_batches = (n_pairs + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  - Epochs: {self.epochs}\")\n",
    "        print(f\"  - Batch size: {self.batch_size}\")\n",
    "        print(f\"  - Total batches per epoch: {n_batches:,}\")\n",
    "        print(f\"  - Learning rate: {self.lr}\")\n",
    "        print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.epochs}\")\n",
    "            self.rng.shuffle(pairs)\n",
    "            total_loss = 0.0\n",
    "\n",
    "            # Progress bar for batches\n",
    "            batch_range = range(0, n_pairs, self.batch_size)\n",
    "            with tqdm(batch_range, desc=f\"  Epoch {epoch}\", unit=\"batch\", \n",
    "                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:\n",
    "                for start in pbar:\n",
    "                    batch = pairs[start:start + self.batch_size]\n",
    "                    centers = batch[:, 0]\n",
    "                    contexts = batch[:, 1]\n",
    "                    batch_loss = self.train_batch(centers, contexts)\n",
    "                    total_loss += batch_loss * len(batch)\n",
    "                    \n",
    "                    # Update progress bar with current loss\n",
    "                    current_avg_loss = total_loss / (start + len(batch))\n",
    "                    pbar.set_postfix({'loss': f'{current_avg_loss:.4f}'})\n",
    "\n",
    "            avg_loss = total_loss / n_pairs\n",
    "            print(f\"  Completed - Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"Training completed!\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "    def train_batch(self, center_idxs, context_idxs):\n",
    "        \"\"\"\n",
    "        Train on a batch of (center, context) pairs using negative sampling.\n",
    "        \"\"\"\n",
    "        B = center_idxs.shape[0]\n",
    "\n",
    "        v_t = self.W_in[center_idxs]       # (B, D)\n",
    "        u_c = self.W_out[context_idxs]     # (B, D)\n",
    "\n",
    "        # positive\n",
    "        score_pos = np.sum(u_c * v_t, axis=1)      # (B,)\n",
    "        sig_pos = self._sigmoid(score_pos)         # (B,)\n",
    "        loss_pos = -np.log(sig_pos + 1e-10)        # (B,)\n",
    "\n",
    "        # negatives\n",
    "        neg_idxs = self.sample_negatives(B)        # (B, K)\n",
    "        u_negs = self.W_out[neg_idxs]              # (B, K, D)\n",
    "        scores_neg = np.einsum(\"bkd,bd->bk\", u_negs, v_t)  # (B, K)\n",
    "        sig_negs = self._sigmoid(-scores_neg)\n",
    "        loss_neg = -np.sum(np.log(sig_negs + 1e-10), axis=1)  # (B,)\n",
    "\n",
    "        loss = np.mean(loss_pos + loss_neg)\n",
    "\n",
    "        # gradients\n",
    "        grad_pos = (1 - sig_pos)                   # (B,)\n",
    "        grad_u_c = grad_pos[:, None] * v_t         # (B, D)\n",
    "        grad_v_pos = grad_pos[:, None] * u_c       # (B, D)\n",
    "\n",
    "        sig_scores_neg = self._sigmoid(scores_neg) # σ(x)\n",
    "        grad_negs = -sig_scores_neg                # (B, K)\n",
    "        grad_u_negs = grad_negs[..., None] * v_t[:, None, :]  # (B, K, D)\n",
    "        grad_v_neg = np.sum(grad_negs[..., None] * u_negs, axis=1)  # (B, D)\n",
    "\n",
    "        grad_v = grad_v_pos + grad_v_neg           # (B, D)\n",
    "\n",
    "        # scatter-add updates\n",
    "        np.add.at(self.W_out, context_idxs, self.lr * grad_u_c)\n",
    "\n",
    "        neg_flat = neg_idxs.reshape(-1)\n",
    "        grad_u_negs_flat = grad_u_negs.reshape(-1, self.embedding_dim)\n",
    "        np.add.at(self.W_out, neg_flat, self.lr * grad_u_negs_flat)\n",
    "\n",
    "        np.add.at(self.W_in, center_idxs, self.lr * grad_v)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # ---------- UTILITIES ----------\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        idx = self.word2idx.get(word)\n",
    "        if idx is None:\n",
    "            raise KeyError(f\"Word '{word}' not in vocabulary.\")\n",
    "        return self.W_in[idx]\n",
    "\n",
    "    def most_similar(self, word, topn=5):\n",
    "        if word not in self.word2idx:\n",
    "            raise KeyError(f\"Word '{word}' not in vocabulary.\")\n",
    "        idx = self.word2idx[word]\n",
    "        v = self.W_in[idx]\n",
    "\n",
    "        norms = np.linalg.norm(self.W_in, axis=1) + 1e-10\n",
    "        sim = (self.W_in @ v) / norms / (np.linalg.norm(v) + 1e-10)\n",
    "\n",
    "        best = np.argsort(-sim)\n",
    "        result = []\n",
    "        for i in best:\n",
    "            if i == idx:\n",
    "                continue\n",
    "            result.append((self.idx2word[i], float(sim[i])))\n",
    "            if len(result) >= topn:\n",
    "                break\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edc80c",
   "metadata": {},
   "source": [
    "Cell 4 – Parameters & load mini wiki corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "834d12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 965518 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the vanilla wiki model\n",
    "corpus_path = Path(\"data/AllCombined.txt\")\n",
    "\n",
    "embedding_dim = 100\n",
    "window_size = 2      # up to 2 words on each side\n",
    "min_count = 5        # ignore very rare words\n",
    "negative_samples = 5\n",
    "learning_rate = 0.025\n",
    "epochs = 3\n",
    "batch_size = 4000\n",
    "\n",
    "# Load corpus\n",
    "sentences = load_corpus(corpus_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c435b5",
   "metadata": {},
   "source": [
    "Cell 5 – Train the vanilla Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f76ad85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Starting Word2Vec Training\n",
      "============================================================\n",
      "\n",
      "[Step 1/4] Building vocabulary...\n",
      "Vocab size: 112970\n",
      "\n",
      "[Step 2/4] Converting sentences to indices...\n",
      "\n",
      "[Step 3/4] Generating skip-gram pairs...\n",
      "Generated 108,994,394 training pairs\n",
      "\n",
      "[Step 4/4] Initializing weights...\n",
      "\n",
      "============================================================\n",
      "Training Configuration:\n",
      "  - Epochs: 3\n",
      "  - Batch size: 4000\n",
      "  - Total batches per epoch: 27,249\n",
      "  - Learning rate: 0.025\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Epoch 1:   4%|▎         | 1008/27249 [00:38<15:59, 27.34batch/s]C:\\Users\\Jack\\AppData\\Local\\Temp\\ipykernel_19040\\2221524560.py:116: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "  Epoch 1: 100%|██████████| 27249/27249 [17:16<00:00, 26.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed - Average loss: 4.7843\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Epoch 2: 100%|██████████| 27249/27249 [17:28<00:00, 25.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed - Average loss: 4.8686\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Epoch 3: 100%|██████████| 27249/27249 [17:17<00:00, 26.25batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed - Average loss: 4.8779\n",
      "\n",
      "============================================================\n",
      "Training completed!\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vanilla_model = Word2VecScratch(\n",
    "    embedding_dim=embedding_dim,\n",
    "    window_size=window_size,\n",
    "    min_count=min_count,\n",
    "    negative_samples=negative_samples,\n",
    "    lr=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "vanilla_model.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45842e",
   "metadata": {},
   "source": [
    "Cell 6 – Quick sanity check: similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b1dcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'dog':\n",
      "fruit                0.6044\n",
      "cat                  0.5902\n",
      "bird                 0.5856\n",
      "bear                 0.5587\n",
      "fish                 0.5574\n",
      "seeds                0.5343\n",
      "tiger                0.5296\n",
      "trees                0.5275\n",
      "leaves               0.5254\n",
      "goat                 0.5100\n"
     ]
    }
   ],
   "source": [
    "test_word = \"dog\"\n",
    "\n",
    "if test_word in vanilla_model.word2idx:\n",
    "    print(f\"Most similar to '{test_word}':\")\n",
    "    for w, score in vanilla_model.most_similar(test_word, topn=10):\n",
    "        print(f\"{w:<20} {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915c2b7",
   "metadata": {},
   "source": [
    "Cell 7 – Export the vanilla Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42776200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings to models\\vanilla_w2v_model.npy\n",
      "Saved vocabulary to models\\vanilla_w2v_model_vocab.txt\n",
      "Saved word2idx to models\\vanilla_w2v_model_word2idx.json\n",
      "Saved full model to models\\vanilla_w2v_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create models/ directory if it doesn't exist\n",
    "export_dir = Path(\"models\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "base_name = \"vanilla_w2v_model\"\n",
    "\n",
    "# 1) Save embeddings matrix\n",
    "embeddings_path = export_dir / f\"{base_name}.npy\"\n",
    "np.save(embeddings_path, vanilla_model.W_in)\n",
    "print(f\"Saved embeddings to {embeddings_path}\")\n",
    "\n",
    "# 2) Save vocabulary list (idx → word)\n",
    "vocab_txt_path = export_dir / f\"{base_name}_vocab.txt\"\n",
    "with vocab_txt_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for w in vanilla_model.idx2word:\n",
    "        f.write(w + \"\\n\")\n",
    "print(f\"Saved vocabulary to {vocab_txt_path}\")\n",
    "\n",
    "# 3) Save word2idx mapping (word → index)\n",
    "vocab_json_path = export_dir / f\"{base_name}_word2idx.json\"\n",
    "with vocab_json_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vanilla_model.word2idx, f, ensure_ascii=False)\n",
    "print(f\"Saved word2idx to {vocab_json_path}\")\n",
    "\n",
    "# 4) Save full model (pickle)\n",
    "model_pkl_path = export_dir / f\"{base_name}.pkl\"\n",
    "with model_pkl_path.open(\"wb\") as f:\n",
    "    pickle.dump(vanilla_model, f)\n",
    "print(f\"Saved full model to {model_pkl_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
