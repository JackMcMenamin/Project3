{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Word2Vec: Vanilla vs RSR (Human Behavioral Similarity)\n",
    "\n",
    "## Experiment Pipeline\n",
    "\n",
    "**Goal**: Compare two Word2Vec models on downstream category prediction:\n",
    "1. **Vanilla**: Trained ONLY on Wikipedia corpus\n",
    "2. **RSR**: Trained on Wikipedia + Human similarity judgments (4.7M triplets)\n",
    "\n",
    "### Pipeline Steps:\n",
    "1. Load Wikipedia corpus\n",
    "2. Load human behavioral similarity matrix  \n",
    "3. Load THINGS concepts & category labels\n",
    "4. Train Word2Vec **VANILLA** (Wikipedia only)\n",
    "5. Train Word2Vec **RSR** (Wikipedia + human similarity)\n",
    "6. Compare both on THINGS category prediction task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "BASE_DATA_DIR = \"data\"\n",
    "\n",
    "# THINGS paths\n",
    "THINGS_CONCEPTS_PATH = os.path.join(BASE_DATA_DIR, \"02_object-level\", \"_concepts-metadata_things.tsv\")\n",
    "PROPERTY_RATINGS_PATH = os.path.join(BASE_DATA_DIR, \"02_object-level\", \"_property-ratings.tsv\")\n",
    "THINGS_FEATURES_PATH = os.path.join(BASE_DATA_DIR, \"03_category-level\", \"category27_manual.tsv\")\n",
    "\n",
    "# Behavioral similarity path\n",
    "BEHAVIORAL_SIM_PATH = os.path.join(BASE_DATA_DIR, \"osfstorage-archive\", \"data\", \"spose_similarity.mat\")\n",
    "\n",
    "# Preprocessed corpus path\n",
    "CORPUS_PATH = os.path.join(BASE_DATA_DIR, \"simplewiki_preprocessed.pkl\")\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Wikipedia Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 2: Loading Wikipedia Corpus\n",
      "======================================================================\n",
      "  Source: Simple Wikipedia (preprocessed)\n",
      "  Sentences: 1,688,343\n",
      "  Sample: ['april', 'apr', 'is', 'the', 'fourth', 'month', 'of', 'the', 'year', 'in']...\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed Simple Wikipedia corpus\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 2: Loading Wikipedia Corpus\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(CORPUS_PATH, 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "print(f\"  Source: Simple Wikipedia (preprocessed)\")\n",
    "print(f\"  Sentences: {len(sentences):,}\")\n",
    "print(f\"  Sample: {sentences[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Human Behavioral Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: Loading Human Behavioral Similarity Matrix\n",
      "======================================================================\n",
      "  Source: 4.7 million human triplet judgments\n",
      "  Matrix shape: (1854, 1854)\n",
      "  Similarity range: [0.052, 1.000]\n",
      "  Mean similarity: 0.334\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 3: Loading Human Behavioral Similarity Matrix\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load full behavioral similarity matrix (1854 x 1854 THINGS concepts)\n",
    "behav_data = sio.loadmat(BEHAVIORAL_SIM_PATH)\n",
    "behav_sim_full = behav_data['spose_sim']\n",
    "\n",
    "print(f\"  Source: 4.7 million human triplet judgments\")\n",
    "print(f\"  Matrix shape: {behav_sim_full.shape}\")\n",
    "print(f\"  Similarity range: [{behav_sim_full.min():.3f}, {behav_sim_full.max():.3f}]\")\n",
    "print(f\"  Mean similarity: {behav_sim_full.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load THINGS Concepts & Category Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: Loading THINGS Concepts & Category Labels\n",
      "======================================================================\n",
      "  THINGS concepts: 1854\n",
      "  Category labels: (1854, 27) (27 binary categories)\n",
      "  Property ratings: (1823, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 4: Loading THINGS Concepts & Category Labels\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load THINGS concepts\n",
    "concepts_df = pd.read_csv(THINGS_CONCEPTS_PATH, sep=\"\\t\")\n",
    "concepts = concepts_df[\"Word\"].tolist()\n",
    "print(f\"  THINGS concepts: {len(concepts)}\")\n",
    "\n",
    "# Load Category27 labels (downstream task)\n",
    "cats_df = pd.read_csv(THINGS_FEATURES_PATH, sep=\"\\t\")\n",
    "feature_cols = cats_df.columns.tolist()\n",
    "Y_all = cats_df[feature_cols].values.astype(np.float32)\n",
    "print(f\"  Category labels: {Y_all.shape} (27 binary categories)\")\n",
    "\n",
    "# Load property ratings (for filtering)\n",
    "prop_df_raw = pd.read_csv(PROPERTY_RATINGS_PATH, sep=\"\\t\")\n",
    "all_num_cols = prop_df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
    "sem_cols = [c for c in all_num_cols if c.endswith(\"_mean\") and not c.startswith(\"N_\") and \"work_time\" not in c]\n",
    "prop_df = prop_df_raw.groupby(\"Word\")[sem_cols].mean()\n",
    "print(f\"  Property ratings: {prop_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Vocabulary & Align Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5: Building Vocabulary & Aligning Data\n",
      "======================================================================\n",
      "\n",
      "Building vocabulary (min_count=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 1688343/1688343 [00:02<00:00, 758686.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Vocabulary size: 104,458\n",
      "  Total tokens: 27,446,826\n",
      "\n",
      "Aligning THINGS concepts with vocabulary...\n",
      "\n",
      "======================================================================\n",
      "ALIGNED DATASET:\n",
      "======================================================================\n",
      "  Valid THINGS concepts: 1386\n",
      "  Category labels Y: (1386, 27)\n",
      "  Similarity matrix: (1386, 1386)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 5: Building Vocabulary & Aligning Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "MIN_COUNT = 5\n",
    "print(f\"\\nBuilding vocabulary (min_count={MIN_COUNT})...\")\n",
    "\n",
    "word_counts = Counter()\n",
    "for sent in tqdm(sentences, desc=\"Counting words\"):\n",
    "    word_counts.update(sent)\n",
    "\n",
    "vocab = sorted([w for w, c in word_counts.items() if c >= MIN_COUNT])\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Compute unigram distribution for negative sampling\n",
    "word_freqs = np.array([word_counts[w] for w in vocab], dtype=np.float32)\n",
    "word_freqs = word_freqs ** 0.75\n",
    "word_probs = word_freqs / word_freqs.sum()\n",
    "\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Total tokens: {sum(word_counts.values()):,}\")\n",
    "\n",
    "# Helper function to find word in vocabulary\n",
    "def get_word_idx(word, word2idx):\n",
    "    candidates = [word, word.lower(), word.replace(\" \", \"_\"), \n",
    "                  word.lower().replace(\" \", \"_\"), word.replace(\" \", \"\"), \n",
    "                  word.lower().replace(\" \", \"\")]\n",
    "    for token in candidates:\n",
    "        if token in word2idx:\n",
    "            return word2idx[token]\n",
    "    return None\n",
    "\n",
    "# Align THINGS concepts with vocabulary\n",
    "print(f\"\\nAligning THINGS concepts with vocabulary...\")\n",
    "valid_concepts = []\n",
    "valid_word_indices = []\n",
    "Y_rows = []\n",
    "valid_things_indices = []\n",
    "\n",
    "for idx, concept in enumerate(concepts):\n",
    "    if concept not in prop_df.index:\n",
    "        continue\n",
    "    word_idx = get_word_idx(concept, word2idx)\n",
    "    if word_idx is None:\n",
    "        continue\n",
    "    valid_word_indices.append(word_idx)\n",
    "    Y_rows.append(Y_all[idx])\n",
    "    valid_concepts.append(concept)\n",
    "    valid_things_indices.append(idx)\n",
    "\n",
    "valid_word_indices = np.array(valid_word_indices)\n",
    "Y = np.stack(Y_rows, axis=0).astype(np.float32)\n",
    "\n",
    "# Extract aligned similarity matrix\n",
    "behav_sim_subset = behav_sim_full[np.ix_(valid_things_indices, valid_things_indices)]\n",
    "behav_sim_target = torch.tensor(behav_sim_subset, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALIGNED DATASET:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Valid THINGS concepts: {len(valid_concepts)}\")\n",
    "print(f\"  Category labels Y: {Y.shape}\")\n",
    "print(f\"  Similarity matrix: {behav_sim_subset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define Model & Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL CONFIGURATION:\n",
      "======================================================================\n",
      "  Embedding dim: 300\n",
      "  Window size: 5\n",
      "  Negative samples: 5\n",
      "  Batch size: 128\n",
      "  Batches per epoch: 10000\n",
      "  Samples per epoch: ~1,280,000\n",
      "  Epochs: 5\n",
      "  Learning rate: 0.001\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class SkipGramWord2Vec(nn.Module):\n",
    "    \"\"\"PyTorch Skip-gram Word2Vec with negative sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.target_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "        self.context_embeddings.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(self, targets, contexts):\n",
    "        t_emb = self.target_embeddings(targets)\n",
    "        c_emb = self.context_embeddings(contexts)\n",
    "        return torch.sum(t_emb * c_emb, dim=1)\n",
    "\n",
    "# ============================================================================\n",
    "# Sample pairs on-the-fly instead of pre-generating 220M pairs...\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_sentences(sentences, word2idx):\n",
    "    \"\"\"Convert sentences to index arrays (do once, reuse).\"\"\"\n",
    "    indexed = []\n",
    "    for sent in tqdm(sentences, desc=\"Indexing sentences\"):\n",
    "        indices = [word2idx[w] for w in sent if w in word2idx]\n",
    "        if len(indices) >= 2:\n",
    "            indexed.append(np.array(indices, dtype=np.int32))\n",
    "    return indexed\n",
    "\n",
    "def sample_batch(indexed_sentences, batch_size, window_size, vocab_size, neg_probs_np):\n",
    "    \"\"\"Sample a batch of (target, context, negatives) on-the-fly.\"\"\"\n",
    "    targets = []\n",
    "    contexts = []\n",
    "    \n",
    "    # Sample random sentences and extract pairs\n",
    "    sent_indices = np.random.randint(0, len(indexed_sentences), batch_size * 2)\n",
    "    \n",
    "    for sent_idx in sent_indices:\n",
    "        sent = indexed_sentences[sent_idx]\n",
    "        if len(sent) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Random position in sentence\n",
    "        pos = np.random.randint(0, len(sent))\n",
    "        target = sent[pos]\n",
    "        \n",
    "        # Random context within window\n",
    "        start = max(0, pos - window_size)\n",
    "        end = min(len(sent), pos + window_size + 1)\n",
    "        context_positions = [j for j in range(start, end) if j != pos]\n",
    "        \n",
    "        if context_positions:\n",
    "            ctx_pos = context_positions[np.random.randint(0, len(context_positions))]\n",
    "            targets.append(target)\n",
    "            contexts.append(sent[ctx_pos])\n",
    "        \n",
    "        if len(targets) >= batch_size:\n",
    "            break\n",
    "    \n",
    "    return np.array(targets[:batch_size]), np.array(contexts[:batch_size])\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "EMBEDDING_DIM = 300\n",
    "WINDOW_SIZE = 5\n",
    "NEG_SAMPLES = 5\n",
    "\n",
    "# Batch size: Typical values are 32-256 for Word2Vec. Larger batches:\n",
    "# - Pros: Better GPU utilisation, more stable gradients, fewer kernel launches\n",
    "# - Cons: Less frequent updates, may need more epochs, less stochasticity\n",
    "# 128 is a good middle ground - standard in practice and still fast\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Number of batches per epoch. With batch_size=128, this gives ~1.28M samples/epoch\n",
    "# Adjust this to control training time vs coverage (more batches = more samples seen)\n",
    "BATCHES_PER_EPOCH = 10000\n",
    "\n",
    "W2V_EPOCHS = 5              \n",
    "W2V_LR = 0.001\n",
    "LAMBDA_RSR = 1.0\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"MODEL CONFIGURATION:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Embedding dim: {EMBEDDING_DIM}\")\n",
    "print(f\"  Window size: {WINDOW_SIZE}\")\n",
    "print(f\"  Negative samples: {NEG_SAMPLES}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches per epoch: {BATCHES_PER_EPOCH}\")\n",
    "print(f\"  Samples per epoch: ~{BATCH_SIZE * BATCHES_PER_EPOCH:,}\")\n",
    "print(f\"  Epochs: {W2V_EPOCHS}\")\n",
    "print(f\"  Learning rate: {W2V_LR}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train VANILLA Word2Vec (Wikipedia Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 7: Training VANILLA Word2Vec (Wikipedia Only)\n",
      "======================================================================\n",
      "Using on-the-fly sampling\n",
      "======================================================================\n",
      "\n",
      "Preprocessing sentences (one-time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing sentences: 100%|██████████| 1688343/1688343 [00:03<00:00, 525042.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Indexed 1,687,117 sentences\n",
      "\n",
      "Training... (~50,000 iterations total)\n",
      "Expected time: ~100 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vanilla Epoch 1/5:  33%|███▎      | 3303/10000 [00:31<01:03, 106.16it/s, loss=0.4513]"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 7: Training VANILLA Word2Vec (Wikipedia Only)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using on-the-fly sampling\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Preprocess sentences ONCE (convert to indices)\n",
    "print(\"Preprocessing sentences (one-time)...\")\n",
    "indexed_sentences = preprocess_sentences(sentences, word2idx)\n",
    "print(f\"  Indexed {len(indexed_sentences):,} sentences\")\n",
    "\n",
    "# Negative sampling distribution (numpy for fast sampling)\n",
    "neg_probs_np = word_probs\n",
    "neg_probs_torch = torch.tensor(word_probs, device=DEVICE)\n",
    "valid_idx_tensor = torch.LongTensor(valid_word_indices).to(DEVICE)\n",
    "\n",
    "# Pre-allocate tensors for speed\n",
    "pos_labels = torch.ones(BATCH_SIZE, device=DEVICE)\n",
    "neg_labels = torch.zeros(BATCH_SIZE * NEG_SAMPLES, device=DEVICE)\n",
    "\n",
    "# Create VANILLA model\n",
    "vanilla_model = SkipGramWord2Vec(vocab_size, EMBEDDING_DIM).to(DEVICE)\n",
    "vanilla_optimizer = optim.Adam(vanilla_model.parameters(), lr=W2V_LR)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"\\nTraining... (~{BATCHES_PER_EPOCH * W2V_EPOCHS:,} iterations total)\")\n",
    "print(f\"Expected time: ~{BATCHES_PER_EPOCH * W2V_EPOCHS // 500} minutes\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(W2V_EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(range(BATCHES_PER_EPOCH), desc=f\"Vanilla Epoch {epoch+1}/{W2V_EPOCHS}\")\n",
    "    for batch_idx in pbar:\n",
    "        # Sample batch on-the-fly (FAST!)\n",
    "        targets_np, contexts_np = sample_batch(\n",
    "            indexed_sentences, BATCH_SIZE, WINDOW_SIZE, vocab_size, neg_probs_np\n",
    "        )\n",
    "        \n",
    "        # To GPU\n",
    "        targets = torch.LongTensor(targets_np).to(DEVICE)\n",
    "        contexts = torch.LongTensor(contexts_np).to(DEVICE)\n",
    "        \n",
    "        # Positive scores\n",
    "        pos_scores = vanilla_model(targets, contexts)\n",
    "        \n",
    "        # Negative samples\n",
    "        neg_contexts = torch.multinomial(neg_probs_torch, len(targets) * NEG_SAMPLES, replacement=True)\n",
    "        neg_targets = targets.repeat_interleave(NEG_SAMPLES)\n",
    "        neg_scores = vanilla_model(neg_targets, neg_contexts)\n",
    "        \n",
    "        # Loss\n",
    "        all_scores = torch.cat([pos_scores, neg_scores])\n",
    "        all_labels = torch.cat([pos_labels[:len(targets)], neg_labels[:len(targets)*NEG_SAMPLES]])\n",
    "        loss = loss_fn(all_scores, all_labels)\n",
    "        \n",
    "        vanilla_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        vanilla_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/BATCHES_PER_EPOCH:.4f}\")\n",
    "\n",
    "# Extract vanilla embeddings\n",
    "X_vanilla = vanilla_model.target_embeddings(valid_idx_tensor).detach().cpu().numpy()\n",
    "print(f\"\\n Vanilla training complete! Embeddings: {X_vanilla.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train RSR Word2Vec (Wikipedia + Human Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 8: Training RSR Word2Vec (Wikipedia + Human Similarity)\n",
      "======================================================================\n",
      "Using FAST on-the-fly sampling + RSR regularization\n",
      "======================================================================\n",
      "\n",
      "Training with RSR every 50 batches...\n",
      "Expected time: ~125 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RSR Epoch 1/5: 100%|██████████| 10000/10000 [06:58<00:00, 23.92it/s, w2v=0.3580, rsr=0.0050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | W2V: 0.3789 | RSR: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RSR Epoch 2/5: 100%|██████████| 10000/10000 [05:44<00:00, 29.02it/s, w2v=0.3482, rsr=0.0054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | W2V: 0.3526 | RSR: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RSR Epoch 3/5: 100%|██████████| 10000/10000 [04:09<00:00, 40.07it/s, w2v=0.3447, rsr=0.0054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | W2V: 0.3461 | RSR: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RSR Epoch 4/5: 100%|██████████| 10000/10000 [04:09<00:00, 40.06it/s, w2v=0.3444, rsr=0.0054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | W2V: 0.3425 | RSR: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RSR Epoch 5/5: 100%|██████████| 10000/10000 [04:12<00:00, 39.56it/s, w2v=0.3371, rsr=0.0054]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | W2V: 0.3401 | RSR: 0.0054\n",
      "\n",
      "✓ RSR training complete! Embeddings: (1386, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 8: Training RSR Word2Vec (Wikipedia + Human Similarity)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Using random sampling + RSR regularisation\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create RSR model (fresh initialisation with random weights)\n",
    "rsr_model = SkipGramWord2Vec(vocab_size, EMBEDDING_DIM).to(DEVICE)\n",
    "rsr_optimizer = optim.Adam(rsr_model.parameters(), lr=W2V_LR)\n",
    "\n",
    "# RSR applied every N batches\n",
    "RSR_EVERY_N_BATCHES = 50\n",
    "\n",
    "print(f\"Training with RSR every {RSR_EVERY_N_BATCHES} batches...\")\n",
    "print(f\"Expected time: ~{BATCHES_PER_EPOCH * W2V_EPOCHS // 400} minutes\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(W2V_EPOCHS):\n",
    "    total_w2v_loss, total_rsr_loss = 0, 0\n",
    "    rsr_count = 0\n",
    "    \n",
    "    pbar = tqdm(range(BATCHES_PER_EPOCH), desc=f\"RSR Epoch {epoch+1}/{W2V_EPOCHS}\")\n",
    "    for batch_idx in pbar:\n",
    "        # Sample batch on-the-fly\n",
    "        targets_np, contexts_np = sample_batch(\n",
    "            indexed_sentences, BATCH_SIZE, WINDOW_SIZE, vocab_size, neg_probs_np\n",
    "        )\n",
    "        \n",
    "        targets = torch.LongTensor(targets_np).to(DEVICE)\n",
    "        contexts = torch.LongTensor(contexts_np).to(DEVICE)\n",
    "        \n",
    "        # Skip-gram loss\n",
    "        pos_scores = rsr_model(targets, contexts)\n",
    "        neg_contexts = torch.multinomial(neg_probs_torch, len(targets) * NEG_SAMPLES, replacement=True)\n",
    "        neg_targets = targets.repeat_interleave(NEG_SAMPLES)\n",
    "        neg_scores = rsr_model(neg_targets, neg_contexts)\n",
    "        \n",
    "        all_scores = torch.cat([pos_scores, neg_scores])\n",
    "        all_labels = torch.cat([pos_labels[:len(targets)], neg_labels[:len(targets)*NEG_SAMPLES]])\n",
    "        L_w2v = loss_fn(all_scores, all_labels)\n",
    "        \n",
    "        # RSR loss (periodically)\n",
    "        L_rsr = torch.tensor(0.0, device=DEVICE)\n",
    "        if batch_idx % RSR_EVERY_N_BATCHES == 0:\n",
    "            things_emb = rsr_model.target_embeddings(valid_idx_tensor)\n",
    "            things_emb_norm = F.normalize(things_emb, p=2, dim=1)\n",
    "            sim_matrix = things_emb_norm @ things_emb_norm.T\n",
    "            L_rsr = F.mse_loss(sim_matrix, behav_sim_target)\n",
    "            rsr_count += 1\n",
    "        \n",
    "        # Combined loss\n",
    "        L_total = L_w2v + LAMBDA_RSR * L_rsr\n",
    "        \n",
    "        rsr_optimizer.zero_grad()\n",
    "        L_total.backward()\n",
    "        rsr_optimizer.step()\n",
    "        \n",
    "        total_w2v_loss += L_w2v.item()\n",
    "        total_rsr_loss += L_rsr.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            pbar.set_postfix({'w2v': f'{L_w2v.item():.4f}', 'rsr': f'{L_rsr.item():.4f}'})\n",
    "    \n",
    "    avg_w2v = total_w2v_loss / BATCHES_PER_EPOCH\n",
    "    avg_rsr = total_rsr_loss / max(1, rsr_count)\n",
    "    print(f\"Epoch {epoch+1} | W2V: {avg_w2v:.4f} | RSR: {avg_rsr:.4f}\")\n",
    "\n",
    "# Extract RSR embeddings\n",
    "X_rsr = rsr_model.target_embeddings(valid_idx_tensor).detach().cpu().numpy()\n",
    "print(f\"\\n RSR training complete! Embeddings: {X_rsr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare on Downstream Task (THINGS Category Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 9: Comparing Models on THINGS Category Prediction\n",
      "======================================================================\n",
      "Task: Predict 27 binary category labels from embeddings\n",
      "Method: Logistic regression with 80/20 train/test split\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "RESULTS: THINGS Category Prediction (F1 Score)\n",
      "======================================================================\n",
      "  VANILLA (Wikipedia only):     F1 = 0.340 ± 0.260\n",
      "  RSR (Wikipedia + Human Sim):  F1 = 0.573 ± 0.253\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "✓ RSR IMPROVED performance by 0.233 F1 points!\n",
      "  Human similarity judgments help category prediction!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STEP 9: Comparing Models on THINGS Category Prediction\")\n",
    "print(\"=\"*70)\n",
    "print(\"Task: Predict 27 binary category labels from embeddings\")\n",
    "print(\"Method: Logistic regression with 80/20 train/test split\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def evaluate_embeddings(X, Y, C=1.0, test_size=0.2, random_state=42):\n",
    "    \"\"\"Evaluate embeddings on category prediction. Returns mean F1.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    f1_scores = []\n",
    "    for f in range(Y.shape[1]):\n",
    "        y = Y[:, f]\n",
    "        if np.all(y == y[0]):\n",
    "            continue\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        clf = LogisticRegression(C=C, max_iter=1000)\n",
    "        clf.fit(X_train, y_train)\n",
    "        f1_scores.append(f1_score(y_test, clf.predict(X_test)))\n",
    "    \n",
    "    return float(np.mean(f1_scores)), float(np.std(f1_scores))\n",
    "\n",
    "# Evaluate both models\n",
    "vanilla_f1, vanilla_std = evaluate_embeddings(X_vanilla, Y)\n",
    "rsr_f1, rsr_std = evaluate_embeddings(X_rsr, Y)\n",
    "\n",
    "# Results\n",
    "print(\"=\"*70)\n",
    "print(\"RESULTS: THINGS Category Prediction (F1 Score)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  VANILLA (Wikipedia only):     F1 = {vanilla_f1:.3f} ± {vanilla_std:.3f}\")\n",
    "print(f\"  RSR (Wikipedia + Human Sim):  F1 = {rsr_f1:.3f} ± {rsr_std:.3f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "delta = rsr_f1 - vanilla_f1\n",
    "print(f\"\\n{'='*70}\")\n",
    "if delta > 0.01:\n",
    "    print(f\"✓ RSR IMPROVED performance by {delta:.3f} F1 points!\")\n",
    "    print(f\"  Human similarity judgments help category prediction!\")\n",
    "elif delta > 0:\n",
    "    print(f\"~ Slight improvement: Δ = {delta:.3f} F1\")\n",
    "else:\n",
    "    print(f\"✗ No improvement: Δ = {delta:.3f} F1\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Nearest Neighbor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NEAREST NEIGHBOR COMPARISON\n",
      "======================================================================\n",
      "\n",
      "=== 'cat' ===\n",
      "VANILLA: ['dog', 'badger', 'mouse', 'owl', 'poodle']\n",
      "RSR:     ['meerkat', 'chipmunk', 'warthog', 'pug', 'mongoose']\n",
      "\n",
      "=== 'dog' ===\n",
      "VANILLA: ['cat', 'poodle', 'pig', 'sled', 'hyena']\n",
      "RSR:     ['poodle', 'meerkat', 'pug', 'alpaca', 'mongoose']\n",
      "\n",
      "=== 'car' ===\n",
      "VANILLA: ['truck', 'limousine', 'motorcycle', 'minivan', 'engine']\n",
      "RSR:     ['minivan', 'jeep', 'limousine', 'sidecar', 'hearse']\n",
      "\n",
      "=== 'hammer' ===\n",
      "VANILLA: ['nail', 'scarecrow', 'goblet', 'anvil', 'stirrup']\n",
      "RSR:     ['pliers', 'chisel', 'screwdriver', 'sledgehammer', 'crowbar']\n",
      "\n",
      "=== 'apple' ===\n",
      "VANILLA: ['blackberry', 'rhubarb', 'lemon', 'laptop', 'juice']\n",
      "RSR:     ['blackberry', 'mango', 'cantaloupe', 'mulberry', 'guacamole']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b) / (norm(a) * norm(b) + 1e-8))\n",
    "\n",
    "def nearest_neighbors(word, embeddings_dict, k=5):\n",
    "    if word not in embeddings_dict:\n",
    "        return []\n",
    "    vec = embeddings_dict[word]\n",
    "    sims = [(w, cosine_sim(vec, v)) for w, v in embeddings_dict.items() if w != word]\n",
    "    return sorted(sims, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# Create word -> embedding dicts\n",
    "vanilla_dict = {w: X_vanilla[i] for i, w in enumerate(valid_concepts)}\n",
    "rsr_dict = {w: X_rsr[i] for i, w in enumerate(valid_concepts)}\n",
    "\n",
    "# Compare nearest neighbors\n",
    "test_words = [\"cat\", \"dog\", \"car\", \"hammer\", \"apple\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEAREST NEIGHBOR COMPARISON\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for word in test_words:\n",
    "    if word not in vanilla_dict:\n",
    "        continue\n",
    "    print(f\"=== '{word}' ===\")\n",
    "    print(\"VANILLA:\", [w for w, _ in nearest_neighbors(word, vanilla_dict, 5)])\n",
    "    print(\"RSR:    \", [w for w, _ in nearest_neighbors(word, rsr_dict, 5)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Models saved to results/\n"
     ]
    }
   ],
   "source": [
    "# Save both models for later use\n",
    "import os\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': vanilla_model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'word2idx': word2idx,\n",
    "    'idx2word': idx2word,\n",
    "}, \"results/vanilla_word2vec.pt\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': rsr_model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'word2idx': word2idx,\n",
    "    'idx2word': idx2word,\n",
    "}, \"results/rsr_word2vec.pt\")\n",
    "\n",
    "print(\"✓ Models saved to results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
