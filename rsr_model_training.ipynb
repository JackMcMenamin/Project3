{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff720e7",
   "metadata": {},
   "source": [
    "Cell 1 – Imports & basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7c63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Import vocab building function from standalone script\n",
    "from get_vocab_info import build_vocab_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296e068",
   "metadata": {},
   "source": [
    "Cell 2 – Load THINGS similarity data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a46e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_things_words(things_words_path: Path):\n",
    "    \"\"\"\n",
    "    Load THINGS word list (one word per line).\n",
    "    Returns list of words.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    with things_words_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word = line.strip()\n",
    "            if word:\n",
    "                words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def load_things_triplets(triplets_path: Path):\n",
    "    \"\"\"\n",
    "    Load THINGS triplet data.\n",
    "    Format: each line has 3 indices: [word1_idx, word2_idx, word3_idx]\n",
    "    where word1 and word2 are similar, word3 is the odd one out.\n",
    "    Returns list of (word1_idx, word2_idx, word3_idx) tuples.\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    with triplets_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                triplets.append((int(parts[0]), int(parts[1]), int(parts[2])))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "def build_things_similarity_matrix(triplets, things_words, vocab_word2idx, vocab_size):\n",
    "    \"\"\"\n",
    "    Build a similarity matrix from THINGS triplets.\n",
    "    For words in our vocabulary that also appear in THINGS:\n",
    "    - If two words appear together as similar pair, increment similarity\n",
    "    - Normalize to get similarity scores\n",
    "    \n",
    "    Returns:\n",
    "    - similarity_matrix: (vocab_size, vocab_size) matrix, 0 for words not in THINGS\n",
    "    - things_word_to_vocab_idx: mapping from THINGS word index to vocab index\n",
    "    \"\"\"\n",
    "    # Build mapping from THINGS words to our vocabulary indices\n",
    "    things_word_to_vocab_idx = {}\n",
    "    for things_idx, things_word in enumerate(things_words):\n",
    "        # Try multiple matching strategies\n",
    "        matched = False\n",
    "        \n",
    "        # Strategy 1: Exact match (lowercase)\n",
    "        normalized_word = things_word.lower()\n",
    "        if normalized_word in vocab_word2idx:\n",
    "            things_word_to_vocab_idx[things_idx] = vocab_word2idx[normalized_word]\n",
    "            matched = True\n",
    "        \n",
    "        # Strategy 2: Replace underscore with space\n",
    "        if not matched:\n",
    "            normalized_word = things_word.lower().replace(\"_\", \" \")\n",
    "            if normalized_word in vocab_word2idx:\n",
    "                things_word_to_vocab_idx[things_idx] = vocab_word2idx[normalized_word]\n",
    "                matched = True\n",
    "        \n",
    "        # Strategy 3: Try without underscores (compound words)\n",
    "        if not matched:\n",
    "            normalized_word = things_word.lower().replace(\"_\", \"\")\n",
    "            if normalized_word in vocab_word2idx:\n",
    "                things_word_to_vocab_idx[things_idx] = vocab_word2idx[normalized_word]\n",
    "                matched = True\n",
    "        \n",
    "        # Strategy 4: Try first word of compound (e.g., \"air_conditioner\" -> \"air\")\n",
    "        if not matched:\n",
    "            parts = things_word.lower().split(\"_\")\n",
    "            if len(parts) > 0 and parts[0] in vocab_word2idx:\n",
    "                things_word_to_vocab_idx[things_idx] = vocab_word2idx[parts[0]]\n",
    "                matched = True\n",
    "    \n",
    "    # Initialize similarity matrix\n",
    "    similarity_counts = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n",
    "    \n",
    "    # Process triplets: word1 and word2 are similar\n",
    "    for word1_idx, word2_idx, word3_idx in triplets:\n",
    "        if word1_idx in things_word_to_vocab_idx and word2_idx in things_word_to_vocab_idx:\n",
    "            v1 = things_word_to_vocab_idx[word1_idx]\n",
    "            v2 = things_word_to_vocab_idx[word2_idx]\n",
    "            # Increment similarity for the similar pair\n",
    "            similarity_counts[v1, v2] += 1.0\n",
    "            similarity_counts[v2, v1] += 1.0\n",
    "    \n",
    "    # Normalize: convert counts to similarity scores (0-1 range)\n",
    "    # Use max count to normalize, or use a sigmoid-like function\n",
    "    max_count = similarity_counts.max()\n",
    "    if max_count > 0:\n",
    "        similarity_matrix = similarity_counts / max_count\n",
    "    else:\n",
    "        similarity_matrix = similarity_counts\n",
    "    \n",
    "    return similarity_matrix, things_word_to_vocab_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04becee",
   "metadata": {},
   "source": [
    "Cell 2.5 : Tokenizer & corpus loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d51c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Very basic tokenizer:\n",
    "    - lowercase\n",
    "    - keep only alphabetic characters and spaces\n",
    "    - split on whitespace\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\\\s]\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def load_corpus(corpus_path: Path):\n",
    "    \"\"\"\n",
    "    Load corpus as list of token lists (one per line).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with corpus_path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens = simple_tokenize(line)\n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "    print(f\"Loaded {len(sentences)} sentences.\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a19a94",
   "metadata": {},
   "source": [
    "Cell 3 – Word2VecRSR class (batched skip-gram + negative sampling + RSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c849d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecRSR:\n",
    "    def __init__( # default hardcoded values\n",
    "        self,\n",
    "        embedding_dim=100,\n",
    "        window_size=2,          # up to 2 words on each side\n",
    "        min_count=1,\n",
    "        negative_samples=5,\n",
    "        lr=0.025,\n",
    "        epochs=3,\n",
    "        batch_size=4,           # mini-batch size\n",
    "        seed=42,\n",
    "        reg_strength=0.1,        # RSR regularization strength (0 = no RSR, 1 = only RSR)\n",
    "        things_similarity_matrix=None,  # THINGS similarity matrix (vocab_size x vocab_size)\n",
    "    ):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.min_count = min_count\n",
    "        self.negative_samples = negative_samples\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.reg_strength = reg_strength\n",
    "        self.things_similarity_matrix = things_similarity_matrix\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # will be set in build_vocab\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.vocab_size = 0\n",
    "\n",
    "        # will be set in init_weights\n",
    "        self.W_in = None   # shape (vocab_size, embedding_dim)\n",
    "        self.W_out = None  # shape (vocab_size, embedding_dim)\n",
    "\n",
    "        # negative sampling distribution\n",
    "        self.neg_sampling_probs = None\n",
    "\n",
    "    # ---------- PREP: VOCAB + TRAINING PAIRS ----------\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of token lists.\n",
    "        \"\"\"\n",
    "        counts = Counter()\n",
    "        for sent in sentences:\n",
    "            counts.update(sent)\n",
    "\n",
    "        # filter by min_count\n",
    "        filtered = [w for w, c in counts.items() if c >= self.min_count]\n",
    "\n",
    "        self.idx2word = sorted(filtered)\n",
    "        self.word2idx = {w: i for i, w in enumerate(self.idx2word)}\n",
    "        self.vocab_size = len(self.idx2word)\n",
    "        print(f\"Vocab size: {self.vocab_size}\")\n",
    "\n",
    "        # build negative sampling distribution: P(w) is proportional to count(w)^0.75\n",
    "        freqs = np.array([counts[w] for w in self.idx2word], dtype=np.float64)\n",
    "        freqs = freqs ** 0.75\n",
    "        self.neg_sampling_probs = freqs / freqs.sum()\n",
    "\n",
    "    def sentences_to_indices(self, sentences):\n",
    "        \"\"\"\n",
    "        Map tokens to indices, dropping out-of-vocab words.\n",
    "        \"\"\"\n",
    "        idx_sentences = []\n",
    "        for sent in sentences:\n",
    "            idxs = [self.word2idx.get(w) for w in sent if w in self.word2idx]\n",
    "            if len(idxs) > 1:\n",
    "                idx_sentences.append(idxs)\n",
    "        return idx_sentences\n",
    "\n",
    "    def generate_skipgram_pairs(self, idx_sentences):\n",
    "        \"\"\"\n",
    "        Generate (center, context) index pairs for skip-gram.\n",
    "        window_size = 2 means up to 2 words on each side of the center word.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        w = self.window_size\n",
    "        for sent in idx_sentences:\n",
    "            n = len(sent)\n",
    "            for i, center in enumerate(sent):\n",
    "                start = max(0, i - w)\n",
    "                end = min(n, i + w + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j == i:\n",
    "                        continue\n",
    "                    context = sent[j]\n",
    "                    pairs.append((center, context))\n",
    "        return pairs\n",
    "\n",
    "    # ---------- MODEL INIT ----------\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialise input and output embeddings.\n",
    "        \"\"\"\n",
    "        self.W_in = 0.01 * self.rng.standard_normal(\n",
    "            (self.vocab_size, self.embedding_dim)\n",
    "        )\n",
    "        self.W_out = 0.01 * self.rng.standard_normal(\n",
    "            (self.vocab_size, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "    # ---------- NEGATIVE SAMPLING + TRAINING ----------\n",
    "\n",
    "    def sample_negatives(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample negative word indices according to the unigram^0.75 distribution.\n",
    "        Returns shape (batch_size, negative_samples).\n",
    "        \"\"\"\n",
    "        return self.rng.choice(\n",
    "            self.vocab_size,\n",
    "            size=batch_size * self.negative_samples,\n",
    "            replace=True,\n",
    "            p=self.neg_sampling_probs,\n",
    "        ).reshape(batch_size, self.negative_samples)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def regularization(self):\n",
    "        \"\"\"\n",
    "        Compute RSR regularization term.\n",
    "        Compares model similarity matrix (from embeddings) with THINGS similarity matrix.\n",
    "        Returns Spearman correlation loss (1 - correlation, so we minimize it).\n",
    "        \"\"\"\n",
    "        if self.things_similarity_matrix is None or self.W_in is None:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute model similarity matrix from embeddings (cosine similarity)\n",
    "        # Normalize embeddings\n",
    "        W_norm = self.W_in / (np.linalg.norm(self.W_in, axis=1, keepdims=True) + 1e-10)\n",
    "        model_sim_matrix = W_norm @ W_norm.T  # (vocab_size, vocab_size)\n",
    "        \n",
    "        # Extract upper triangular parts (excluding diagonal) for comparison\n",
    "        # Get indices for upper triangle\n",
    "        triu_indices = np.triu_indices(self.vocab_size, k=1)\n",
    "        model_sim_flat = model_sim_matrix[triu_indices]\n",
    "        things_sim_flat = self.things_similarity_matrix[triu_indices]\n",
    "        \n",
    "        # Only consider pairs where THINGS similarity is defined (non-zero)\n",
    "        # This focuses on words that appear in THINGS dataset\n",
    "        valid_mask = things_sim_flat > 0\n",
    "        if valid_mask.sum() == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        model_sim_valid = model_sim_flat[valid_mask]\n",
    "        things_sim_valid = things_sim_flat[valid_mask]\n",
    "        \n",
    "        # Compute Spearman correlation\n",
    "        if len(model_sim_valid) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            corr, _ = spearmanr(model_sim_valid, things_sim_valid)\n",
    "            if np.isnan(corr):\n",
    "                return 0.0\n",
    "            # Return 1 - correlation (so we minimize it, higher correlation = lower loss)\n",
    "            return 1.0 - corr\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def train(self, sentences):\n",
    "        \"\"\"\n",
    "        High-level training:\n",
    "        - build vocab\n",
    "        - convert sentences to indices\n",
    "        - generate skip-gram pairs\n",
    "        - train using negative sampling with mini-batches\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Starting Word2Vec RSR Training\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\n[Step 1/4] Building vocabulary...\")\n",
    "        self.build_vocab(sentences)\n",
    "        \n",
    "        print(\"\\n[Step 2/4] Converting sentences to indices...\")\n",
    "        idx_sentences = self.sentences_to_indices(sentences)\n",
    "        \n",
    "        print(\"\\n[Step 3/4] Generating skip-gram pairs...\")\n",
    "        pairs = self.generate_skipgram_pairs(idx_sentences)\n",
    "        print(f\"Generated {len(pairs):,} training pairs\")\n",
    "\n",
    "        print(\"\\n[Step 4/4] Initializing weights...\")\n",
    "        self.init_weights()\n",
    "\n",
    "        pairs = np.array(pairs, dtype=np.int64)\n",
    "        n_pairs = len(pairs)\n",
    "        n_batches = (n_pairs + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  - Epochs: {self.epochs}\")\n",
    "        print(f\"  - Batch size: {self.batch_size}\")\n",
    "        print(f\"  - Total batches per epoch: {n_batches:,}\")\n",
    "        print(f\"  - Learning rate: {self.lr}\")\n",
    "        print(f\"  - RSR regularization strength: {self.reg_strength}\")\n",
    "        print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.epochs}\")\n",
    "            self.rng.shuffle(pairs)\n",
    "            total_loss = 0.0\n",
    "            total_rsr_loss = 0.0\n",
    "            batch_count = 0\n",
    "\n",
    "            # Progress bar for batches\n",
    "            batch_range = range(0, n_pairs, self.batch_size)\n",
    "            with tqdm(batch_range, desc=f\"  Epoch {epoch}\", unit=\"batch\", \n",
    "                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:\n",
    "                for start in pbar:\n",
    "                    batch = pairs[start:start + self.batch_size]\n",
    "                    centers = batch[:, 0]\n",
    "                    contexts = batch[:, 1]\n",
    "                    batch_loss, rsr_loss = self.train_batch(centers, contexts)\n",
    "                    total_loss += batch_loss * len(batch)\n",
    "                    total_rsr_loss += rsr_loss\n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    # Update progress bar with current losses\n",
    "                    current_avg_loss = total_loss / (start + len(batch))\n",
    "                    current_avg_rsr_loss = total_rsr_loss / batch_count\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{current_avg_loss:.4f}',\n",
    "                        'rsr': f'{current_avg_rsr_loss:.4f}'\n",
    "                    })\n",
    "\n",
    "            avg_loss = total_loss / n_pairs\n",
    "            avg_rsr_loss = total_rsr_loss / max(1, n_pairs // self.batch_size)\n",
    "            print(f\"  Completed - Average loss: {avg_loss:.4f}, Average RSR loss: {avg_rsr_loss:.4f}\")\n",
    "\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"Training completed!\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "    def train_batch(self, center_idxs, context_idxs):\n",
    "        \"\"\"\n",
    "        Train on a batch of (center, context) pairs using negative sampling.\n",
    "        Returns: (skipgram_loss, rsr_loss)\n",
    "        \"\"\"\n",
    "        B = center_idxs.shape[0]\n",
    "\n",
    "        v_t = self.W_in[center_idxs]       # (B, D)\n",
    "        u_c = self.W_out[context_idxs]     # (B, D)\n",
    "\n",
    "        # positive\n",
    "        score_pos = np.sum(u_c * v_t, axis=1)      # (B,)\n",
    "        sig_pos = self._sigmoid(score_pos)         # (B,)\n",
    "        loss_pos = -np.log(sig_pos + 1e-10)        # (B,)\n",
    "\n",
    "        # negatives\n",
    "        neg_idxs = self.sample_negatives(B)        # (B, K)\n",
    "        u_negs = self.W_out[neg_idxs]              # (B, K, D)\n",
    "        scores_neg = np.einsum(\"bkd,bd->bk\", u_negs, v_t)  # (B, K)\n",
    "        sig_negs = self._sigmoid(-scores_neg)\n",
    "        loss_neg = -np.sum(np.log(sig_negs + 1e-10), axis=1)  # (B,)\n",
    "\n",
    "        skipgram_loss = np.mean(loss_pos + loss_neg)\n",
    "\n",
    "        # gradients for skipgram loss\n",
    "        grad_pos = (1 - sig_pos)                   # (B,)\n",
    "        grad_u_c = grad_pos[:, None] * v_t         # (B, D)\n",
    "        grad_v_pos = grad_pos[:, None] * u_c       # (B, D)\n",
    "\n",
    "        sig_scores_neg = self._sigmoid(scores_neg) # σ(x)\n",
    "        grad_negs = -sig_scores_neg                # (B, K)\n",
    "        grad_u_negs = grad_negs[..., None] * v_t[:, None, :]  # (B, K, D)\n",
    "        grad_v_neg = np.sum(grad_negs[..., None] * u_negs, axis=1)  # (B, D)\n",
    "\n",
    "        grad_v = grad_v_pos + grad_v_neg           # (B, D)\n",
    "\n",
    "        # RSR regularization (computed periodically, not every batch for efficiency)\n",
    "        # We'll compute it less frequently to avoid overhead\n",
    "        rsr_loss = 0.0\n",
    "        if self.reg_strength > 0 and self.things_similarity_matrix is not None:\n",
    "            # Compute RSR loss periodically (every 10 batches to reduce overhead)\n",
    "            # Store batch count as instance variable if needed, or use a simpler approach\n",
    "            # For now, compute it every batch (can be optimized later)\n",
    "            rsr_loss = self.regularization()\n",
    "            \n",
    "            # Approximate gradient for RSR: use a simple approach\n",
    "            # Since Spearman correlation is not directly differentiable,\n",
    "            # we approximate by using the gradient of MSE between similarity matrices\n",
    "            # This pushes model similarity towards THINGS similarity\n",
    "            if rsr_loss > 0:\n",
    "                # Compute model similarity matrix\n",
    "                W_norm = self.W_in / (np.linalg.norm(self.W_in, axis=1, keepdims=True) + 1e-10)\n",
    "                model_sim = W_norm @ W_norm.T\n",
    "                \n",
    "                # Compute gradient approximation: push model similarity towards THINGS similarity\n",
    "                # This is a simplified gradient - full implementation would use soft ranking\n",
    "                diff = model_sim - self.things_similarity_matrix\n",
    "                rsr_grad_scale = self.reg_strength * 0.01  # Small scale for stability\n",
    "                \n",
    "                # Approximate gradient: adjust embeddings to reduce difference\n",
    "                # For each word in batch, compute gradient contribution\n",
    "                for b_idx, c_idx in enumerate(center_idxs):\n",
    "                    # Gradient approximation: push this word's similarity towards THINGS similarity\n",
    "                    grad_rsr_approx = rsr_grad_scale * np.sum(\n",
    "                        diff[c_idx, :, None] * W_norm, axis=0\n",
    "                    ) / max(1, np.sum(np.abs(diff[c_idx, :])))\n",
    "                    grad_v[b_idx] += grad_rsr_approx\n",
    "\n",
    "        # Combined loss (for logging)\n",
    "        total_loss = (1 - self.reg_strength) * skipgram_loss + self.reg_strength * rsr_loss\n",
    "\n",
    "        # scatter-add updates\n",
    "        np.add.at(self.W_out, context_idxs, self.lr * grad_u_c)\n",
    "\n",
    "        neg_flat = neg_idxs.reshape(-1)\n",
    "        grad_u_negs_flat = grad_u_negs.reshape(-1, self.embedding_dim)\n",
    "        np.add.at(self.W_out, neg_flat, self.lr * grad_u_negs_flat)\n",
    "\n",
    "        np.add.at(self.W_in, center_idxs, self.lr * grad_v)\n",
    "\n",
    "        return skipgram_loss, rsr_loss\n",
    "\n",
    "    # ---------- UTILITIES ----------\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        idx = self.word2idx.get(word)\n",
    "        if idx is None:\n",
    "            raise KeyError(f\"Word '{word}' not in vocabulary.\")\n",
    "        return self.W_in[idx]\n",
    "\n",
    "    def most_similar(self, word, topn=5):\n",
    "        if word not in self.word2idx:\n",
    "            raise KeyError(f\"Word '{word}' not in vocabulary.\")\n",
    "        idx = self.word2idx[word]\n",
    "        v = self.W_in[idx]\n",
    "\n",
    "        norms = np.linalg.norm(self.W_in, axis=1) + 1e-10\n",
    "        sim = (self.W_in @ v) / norms / (np.linalg.norm(v) + 1e-10)\n",
    "\n",
    "        best = np.argsort(-sim)\n",
    "        result = []\n",
    "        for i in best:\n",
    "            if i == idx:\n",
    "                continue\n",
    "            result.append((self.idx2word[i], float(sim[i])))\n",
    "            if len(result) >= topn:\n",
    "                break\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edc80c",
   "metadata": {},
   "source": [
    "Cell 4 – Parameters & load mini wiki corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834d12d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 965518 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the RSR Word2Vec model\n",
    "corpus_path = Path(\"data/AllCombined.txt\")\n",
    "\n",
    "embedding_dim = 100\n",
    "window_size = 2      # up to 2 words on each side\n",
    "min_count = 5        # ignore very rare words\n",
    "negative_samples = 5\n",
    "learning_rate = 0.025\n",
    "epochs = 3\n",
    "batch_size = 4\n",
    "reg_strength = 0.1   # RSR regularization strength (0 = no RSR, 1 = only RSR)\n",
    "\n",
    "# Load corpus\n",
    "sentences = load_corpus(corpus_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c435b5",
   "metadata": {},
   "source": [
    "Cell 5 – Load THINGS similarity data and train RSR Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f76ad85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Preparing RSR Word2Vec Model\n",
      "============================================================\n",
      "\n",
      "[Step 1/5] Building vocabulary from corpus...\n",
      "✓ Vocab size: 112,970\n",
      "\n",
      "[Step 2/5] Loading THINGS words...\n",
      "✓ Loaded 1,854 THINGS words\n",
      "\n",
      "[Step 3/5] Loading THINGS triplets...\n",
      "✓ Loaded 4,120,663 triplets\n",
      "\n",
      "[Step 4/5] Building THINGS similarity matrix...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 95.1 GiB for an array with shape (112970, 112970) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(triplets)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m triplets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Step 4/5] Building THINGS similarity matrix...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m things_sim_matrix, things_word_to_vocab_idx = \u001b[43mbuild_things_similarity_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtriplets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthings_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Built similarity matrix with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.sum(things_sim_matrix\u001b[38;5;250m \u001b[39m>\u001b[38;5;250m \u001b[39m\u001b[32m0\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m non-zero pairs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Mapped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(things_word_to_vocab_idx)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m THINGS words to vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mbuild_things_similarity_matrix\u001b[39m\u001b[34m(triplets, things_words, vocab_word2idx, vocab_size)\u001b[39m\n\u001b[32m     73\u001b[39m             matched = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Initialize similarity matrix\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m similarity_counts = np.zeros((vocab_size, vocab_size), dtype=np.float64)\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# Process triplets: word1 and word2 are similar\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word1_idx, word2_idx, word3_idx \u001b[38;5;129;01min\u001b[39;00m triplets:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 95.1 GiB for an array with shape (112970, 112970) and data type float64"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Preparing RSR Word2Vec Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build vocabulary first (without creating a model)\n",
    "print(\"\\n[Step 1/5] Building vocabulary from corpus...\")\n",
    "word2idx, idx2word, vocab_size = build_vocab_info(sentences, min_count=min_count)\n",
    "print(f\"✓ Vocab size: {vocab_size:,}\")\n",
    "\n",
    "# Load THINGS data\n",
    "things_words_path = Path(\"things_similarity/variables/unique_id.txt\")\n",
    "things_triplets_path = Path(\"things_similarity/data/triplet_dataset/trainset.txt\")\n",
    "\n",
    "print(\"\\n[Step 2/5] Loading THINGS words...\")\n",
    "things_words = load_things_words(things_words_path)\n",
    "print(f\"✓ Loaded {len(things_words):,} THINGS words\")\n",
    "\n",
    "# Load triplets and build similarity matrix\n",
    "print(\"\\n[Step 3/5] Loading THINGS triplets...\")\n",
    "triplets = load_things_triplets(things_triplets_path)\n",
    "print(f\"✓ Loaded {len(triplets):,} triplets\")\n",
    "\n",
    "print(\"\\n[Step 4/5] Building THINGS similarity matrix...\")\n",
    "things_sim_matrix, things_word_to_vocab_idx = build_things_similarity_matrix(\n",
    "    triplets, things_words, word2idx, vocab_size\n",
    ")\n",
    "print(f\"✓ Built similarity matrix with {np.sum(things_sim_matrix > 0):,} non-zero pairs\")\n",
    "print(f\"✓ Mapped {len(things_word_to_vocab_idx):,} THINGS words to vocabulary\")\n",
    "\n",
    "print(\"\\n[Step 5/5] Creating RSR model...\")\n",
    "# Create RSR model with similarity matrix\n",
    "rsr_model = Word2VecRSR(\n",
    "    embedding_dim=embedding_dim,\n",
    "    window_size=window_size,\n",
    "    min_count=min_count,\n",
    "    negative_samples=negative_samples,\n",
    "    lr=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    reg_strength=reg_strength,\n",
    "    things_similarity_matrix=things_sim_matrix,\n",
    ")\n",
    "print(\"✓ Model created\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\n\")\n",
    "rsr_model.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45842e",
   "metadata": {},
   "source": [
    "Cell 6 – Quick sanity check: similar words (RSR model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1dcc07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vanilla_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_word = \u001b[33m\"\u001b[39m\u001b[33mdog\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvanilla_model\u001b[49m.word2idx:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMost similar to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m w, score \u001b[38;5;129;01min\u001b[39;00m vanilla_model.most_similar(test_word, topn=\u001b[32m10\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'vanilla_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_word = \"dog\"\n",
    "\n",
    "if test_word in rsr_model.word2idx:\n",
    "    print(f\"Most similar to '{test_word}':\")\n",
    "    for w, score in rsr_model.most_similar(test_word, topn=10):\n",
    "        print(f\"{w:<20} {score:.4f}\")\n",
    "else:\n",
    "    print(f\"'{test_word}' not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915c2b7",
   "metadata": {},
   "source": [
    "Cell 7 – Export the RSR Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42776200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models/ directory if it doesn't exist\n",
    "export_dir = Path(\"models\")\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "base_name = f\"rsr_w2v_model_reg{reg_strength}\"\n",
    "\n",
    "# 1) Save embeddings matrix\n",
    "embeddings_path = export_dir / f\"{base_name}.npy\"\n",
    "np.save(embeddings_path, rsr_model.W_in)\n",
    "print(f\"Saved embeddings to {embeddings_path}\")\n",
    "\n",
    "# 2) Save vocabulary list (idx → word)\n",
    "vocab_txt_path = export_dir / f\"{base_name}_vocab.txt\"\n",
    "with vocab_txt_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for w in rsr_model.idx2word:\n",
    "        f.write(w + \"\\n\")\n",
    "print(f\"Saved vocabulary to {vocab_txt_path}\")\n",
    "\n",
    "# 3) Save word2idx mapping (word → index)\n",
    "vocab_json_path = export_dir / f\"{base_name}_word2idx.json\"\n",
    "with vocab_json_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rsr_model.word2idx, f, ensure_ascii=False)\n",
    "print(f\"Saved word2idx to {vocab_json_path}\")\n",
    "\n",
    "# 4) Save full model (pickle)\n",
    "model_pkl_path = export_dir / f\"{base_name}.pkl\"\n",
    "with model_pkl_path.open(\"wb\") as f:\n",
    "    pickle.dump(rsr_model, f)\n",
    "print(f\"Saved full model to {model_pkl_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
